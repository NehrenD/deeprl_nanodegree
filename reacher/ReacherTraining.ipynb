{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.replay_buffer import ReplayBuffer\n",
    "from utils.ou_noise import OUNoise\n",
    "from networks.ddpg_actor import DDPGActor\n",
    "from networks.ddpg_critic import DDPGCritic\n",
    "\n",
    "from utils.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent():\n",
    "    \n",
    "    def __init__(self,state_size, action_size, random_seed):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        # Actor Networks\n",
    "        self.actor_local = DDPGActor(state_size, action_size, random_seed,ACTOR_FC1_UNITS,ACTOR_FC2_UNITS).to(device)\n",
    "        self.actor_target = DDPGActor(state_size, action_size, random_seed,ACTOR_FC1_UNITS,ACTOR_FC2_UNITS).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Networks \n",
    "        self.critic_local = DDPGCritic(state_size, action_size, random_seed,CRITIC_FC1_UNITS,CRITIC_FC2_UNITS).to(device)\n",
    "        self.critic_target = DDPGCritic(state_size, action_size, random_seed,CRITIC_FC1_UNITS,CRITIC_FC2_UNITS).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        self.noise = OUNoise(action_size,random_seed,NOISE_THETA,NOISE_SIGMA)\n",
    "        \n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "                \n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    def learning_step(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "                \n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions.float())\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Reacher-2.app',no_graphics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "state_size = brain.vector_observation_space_size\n",
    "action_size = brain.vector_action_space_size\n",
    "num_agents = 20\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "n_epochs = 5000\n",
    "avg_score_target = 10\n",
    "avg_score_runs = 100\n",
    "max_steps = 750\n",
    "print_every = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "\n",
    "NOISE_THETA = 0.15\n",
    "NOISE_SIGMA = 0.2\n",
    "\n",
    "LR_ACTOR = 1e-4\n",
    "LR_CRITIC = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(state_size,action_size,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_deque_10 = deque(maxlen=10)\n",
    "scores_deque_50 = deque(maxlen=50)\n",
    "scores_deque_100 = deque(maxlen=100)\n",
    "scores = []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    states = env_info.vector_observations\n",
    "    agent.reset()\n",
    "    \n",
    "    total_scores = np.zeros(num_agents)\n",
    "    \n",
    "    for t in range(max_steps):\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "               \n",
    "        total_scores += rewards\n",
    "        \n",
    "        _ = [agent.add_to_memory(states[i],actions[i],rewards[i],next_states[i],dones[i]) for i in range(num_agents)]        \n",
    "        \n",
    "        if t % 20 == 0:\n",
    "            for _ in range(10):\n",
    "                agent.learning_step()\n",
    "        \n",
    "        states = next_states\n",
    "    \n",
    "    \n",
    "    \n",
    "    scores_deque_10.extend(total_scores)\n",
    "    scores_deque_50.extend(total_scores)\n",
    "    scores_deque_100.extend(total_scores)\n",
    "    scores.extend(total_scores)\n",
    "        \n",
    "    if epoch % print_every == 0:\n",
    "        \n",
    "        avg_scores_10 = np.asanyarray(scores_deque_10).mean()\n",
    "        avg_scores_50 = np.asanyarray(scores_deque_50).mean()\n",
    "        avg_scores_100 = np.asanyarray(scores_deque_100).mean()\n",
    "        \n",
    "        print(f\"Epoch: {epoch} \\tavg_score_10: {avg_scores_10}\\tavg_score_50: {avg_scores_50}\\tavg_score_100: {avg_scores_100}\")\n",
    "        if avg_scores_100 > avg_score_runs:\n",
    "            print(\"Enviroment Solved!\")\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_grid = {\n",
    "\n",
    "        #NETWORK\n",
    "        'FC1_UNITS' : [32,64,128],\n",
    "        'FC2_UNITS': [32, 64, 128],\n",
    "        'ACTOR_LR': [1e-2, 1e-3,1e-4],\n",
    "        'CRITIC_LR': [1e-2, 1e-3, 1e-4],\n",
    "        'TAU': [1e-2, 1e-3, 1e-4],\n",
    "        'NOISE_SIGMA': [0.1, 0.3, 0.5],\n",
    "\n",
    "        #REPLAY BUFF\n",
    "        'BUFFER_SIZE': [1e4,1e5,1e6],\n",
    "        'BATCH_SIZE': [64, 128, 256],\n",
    "\n",
    "        'MAX_STEPS': [ 200,300,400],\n",
    "        'TRAIN_STEP': [10,50,100],\n",
    "        'TRAIN_TIME': [10, 50, 100],\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = np.array([np.random.choice([0,1,2],200) for _ in range(len(config_grid.keys()))])\n",
    "experiments = np.unique(experiments,axis=1)[:,:100].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(config_grid.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 11)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = experiments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = []\n",
    "for i in range(100):\n",
    "    e = experiments[i]\n",
    "    c = { k:config_grid[k][e[i]] for i,k in enumerate(keys)}\n",
    "    configs.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = np.unique(np.array([[1,1,1,4],[1,1,1,4],[2,1,1,1],[2,2,2,2],[2,2,2,2]]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = 10000\n",
    "sumv = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16666.666666666668"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5000/0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = open('./reacher.log').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score_line = [s for s in log_data if \"max_score: \" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scores = [float(s.strip().split()[-1]) for s in max_score_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
