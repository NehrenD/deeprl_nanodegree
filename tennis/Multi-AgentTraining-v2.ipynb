{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from collections import deque,namedtuple\n",
    "import copy\n",
    "import ptan\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maddpg_model import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size,device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience( state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.array([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array([e.action for e in experiences if e is not None])).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.array([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array([e.done for e in experiences if e is not None]).astype('uint8')).to(self.device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name='Tennis.app',no_graphics=True)\n",
    "env = UnityEnvironment(file_name='Tennis_Linux/Tennis.x86_64',no_graphics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "n_agents = len(env_info.agents)\n",
    "state_size = env_info.vector_observations.shape[1]\n",
    "action_size = env_info.previous_vector_actions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "writer = SummaryWriter(comment=\"-tennis_maddpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.ACTOR_FC1_UNITS = 128\n",
    "config.ACTOR_FC2_UNITS = 64\n",
    "config.CRITIC_FC1_UNITS = 128\n",
    "config.CRITIC_FC2_UNITS = 64\n",
    "config.NOISE_THETA = 0.15\n",
    "config.NOISE_SIGMA = 0.2\n",
    "config.LR_ACTOR = 1e-4\n",
    "config.LR_CRITIC = 3e-4\n",
    "config.TAU = 1e-4\n",
    "\n",
    "#REPLAY BUFFER\n",
    "config.BUFFER_SIZE = int(1e6)\n",
    "config.BATCH_SIZE = 512\n",
    "config.GAMMA = 0.99\n",
    "config.WEIGHT_DECAY = 0\n",
    "config.device = 'cpu'\n",
    "\n",
    "seed = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, n_agents, state_size, action_size, seed, fcs1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(n_agents*state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+n_agents*action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class DDPGAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents, state_size, action_size, random_seed,config):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed,config.ACTOR_FC1_UNITS,config.ACTOR_FC2_UNITS).to(self.device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed,config.ACTOR_FC1_UNITS,config.ACTOR_FC2_UNITS).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=config.LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(n_agents,state_size, action_size, random_seed,config.CRITIC_FC1_UNITS,config.CRITIC_FC2_UNITS).to(self.device)\n",
    "        self.critic_target = Critic(n_agents, state_size, action_size, random_seed,config.CRITIC_FC1_UNITS,config.CRITIC_FC2_UNITS).to(self.device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=config.LR_CRITIC, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "    \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    \n",
    "    def critic_training_step(self,full_states,full_actions, full_next_states, full_next_actions, rewards,dones):        \n",
    "        q_target_next = self.critic_target(full_next_states, full_next_actions)\n",
    "        q_targets = rewards + (self.config.GAMMA * q_target_next * (1 - dones))\n",
    "        q_expected = self.critic_local(full_states, full_actions)\n",
    "        \n",
    "        critic_loss = F.mse_loss(q_expected, q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return critic_loss\n",
    "        \n",
    "     \n",
    "    def actor_training_step(self,full_states,full_actions):\n",
    "        \n",
    "        actor_loss = -self.critic_local(full_states, full_actions).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        return actor_loss\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [DDPGAgent(n_agents,state_size,action_size,seed,config) for _ in range(n_agents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(config.BUFFER_SIZE,config.BATCH_SIZE,config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_replay_buffer(agents,n_traj):\n",
    "    \n",
    "    for _ in range(n_traj):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        for step in range(300):\n",
    "            states = env_info.vector_observations\n",
    "            \n",
    "            actions = [agents[i].act(states[i]) for i in range(len(agents))]\n",
    "            env_info= env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            dones = env_info.local_done\n",
    "            rewards = env_info.rewards            \n",
    "\n",
    "            replay_buffer.add(states,actions,rewards,next_states,dones)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if np.any(dones):                                \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay_buffer = ReplayBuffer(config.BUFFER_SIZE,5,config.device)\n",
    "#populate_replay_buffer(agents,5)\n",
    "#states,actions,rewards,next_states,dones = replay_buffer.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fs_v = states.view(-1,n_agents*state_size)\n",
    "#fa_v = actions.view(-1,n_agents*action_size)\n",
    "#fns_v = next_states.view(-1,n_agents*state_size)\n",
    "#r_v = rewards.view(-1,n_agents,1)\n",
    "#d_v = dones.view(-1,n_agents,1)\n",
    "#next_actions = [agents[i].act(next_states[:,i,:].detach().numpy()) for i in range(len(agents))]\n",
    "#fna_v = torch.from_numpy(np.concatenate(next_actions,axis=1)).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = [agents[i].critic_training_step(fs_v,fa_v, fns_v, fna_v, r_v[:,i,:], d_v[:,i,:]) for i in range(n_agents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = [agents[i].actor_training_step(fs_v,fa_v) for i in range(n_agents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 reward_100: 0.0\n",
      "Epoch: 200 reward_100: 0.0\n",
      "Epoch: 300 reward_100: 0.0\n",
      "Epoch: 400 reward_100: 0.0\n",
      "Epoch: 500 reward_100: 0.0\n",
      "Epoch: 600 reward_100: 0.0\n",
      "Epoch: 700 reward_100: 0.0\n",
      "Epoch: 800 reward_100: 0.0\n",
      "Epoch: 897 reward: 0.0\r"
     ]
    }
   ],
   "source": [
    "rewards_history = []\n",
    "stop_score = 0.5\n",
    "\n",
    "num_epochs = 50000\n",
    "max_steps = 300\n",
    "WAIT_EPOCHS = 300\n",
    "\n",
    "with ptan.common.utils.TBMeanTracker(writer, batch_size=100) as tb_tracker:\n",
    "        \n",
    "    for epoch in range(1,num_epochs):\n",
    "\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        tot_reward = np.zeros(n_agents)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            states = env_info.vector_observations\n",
    "            actions = [agents[i].act(states[i]) for i in range(len(agents))]\n",
    "            env_info= env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            dones = env_info.local_done\n",
    "            rewards = env_info.rewards\n",
    "            tot_reward += rewards\n",
    "\n",
    "            replay_buffer.add(states,actions,rewards,next_states,dones)\n",
    "\n",
    "            if np.any(dones):\n",
    "                max_reward = tot_reward.max()\n",
    "                tb_tracker.track(\"reward\", max_reward, epoch)               \n",
    "                rewards_history.append(max_reward)\n",
    "                \n",
    "                reward_100 = np.mean(rewards_history[-100:])\n",
    "                tb_tracker.track(\"reward_100\", reward_100, epoch)\n",
    "                \n",
    "                print(f'Epoch: {epoch} reward: {max_reward}',end='\\r')\n",
    "\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f'Epoch: {epoch} reward_100: {reward_100}')\n",
    "                \n",
    "                break\n",
    "\n",
    "            if len(replay_buffer) > config.BATCH_SIZE and epoch > WAIT_EPOCHS:\n",
    "            \n",
    "                states,actions,rewards,next_states,dones = replay_buffer.sample()\n",
    "                \n",
    "                fs_v = states.view(-1,n_agents*state_size)\n",
    "                fa_v = actions.view(-1,n_agents*action_size)\n",
    "                fns_v = next_states.view(-1,n_agents*state_size)\n",
    "                r_v = rewards.view(-1,n_agents,1)\n",
    "                d_v = dones.view(-1,n_agents,1)\n",
    "                next_actions = [agents[i].act(next_states[:,i,:].detach().numpy()) for i in range(len(agents))]\n",
    "                fna_v = torch.from_numpy(np.concatenate(next_actions,axis=1)).to(config.device)\n",
    "                \n",
    "                critics_loss = [agents[i].critic_training_step(fs_v,fa_v, fns_v, fna_v, r_v[:,i,:], d_v[:,i,:]) for i in range(n_agents)]\n",
    "                actors_loss = [agents[i].actor_training_step(fs_v,fa_v) for i in range(n_agents)]\n",
    "                                \n",
    "                _ = [tb_tracker.track(f\"loss_critic_{i}\", critics_loss[i], epoch) for i in range(n_agents)]\n",
    "                _ = [tb_tracker.track(f\"loss_actor_{i}\", actors_loss[i], epoch)for i in range(n_agents)]\n",
    "        \n",
    "        if len(rewards_history) > 100 and reward_100 > stop_score:\n",
    "            print(f'Solved. Episode {epoch}, mean reward {reward_100}')\n",
    "            break\n",
    "\n",
    "\n",
    "    learning_state = {\n",
    "            'actor': actor_net.state_dict(),\n",
    "            'critic': critic_net.state_dict(),\n",
    "            'act_opt': actor_optimizer.state_dict(),\n",
    "            'crt_opt': critic_optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'rewards_history': rewards_history\n",
    "    }\n",
    "\n",
    "    torch.save(learning_state,'./partial_state.ckp')\n",
    "\n",
    "    writer.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_state = {\n",
    "        'actor': actor_net.state_dict(),\n",
    "        'critic': critic_net.state_dict(),\n",
    "        'act_opt': actor_optimizer.state_dict(),\n",
    "        'crt_opt': critic_optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'rewards_history': rewards_history\n",
    "}\n",
    "    \n",
    "torch.save(learning_state,'./partial_state.ckp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
