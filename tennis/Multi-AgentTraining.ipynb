{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from collections import deque,namedtuple\n",
    "import copy\n",
    "import ptan\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maddpg_model import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size,device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience( state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.array([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array([e.action for e in experiences if e is not None])).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.array([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array([e.done for e in experiences if e is not None]).astype('uint8')).to(self.device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Tennis.app',no_graphics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "n_agents = len(env_info.agents)\n",
    "state_size = env_info.vector_observations.shape[1]\n",
    "action_size = env_info.previous_vector_actions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "writer = SummaryWriter(comment=\"-tennis_maddpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.ACTOR_FC1_UNITS = 128\n",
    "config.ACTOR_FC2_UNITS = 64\n",
    "config.CRITIC_FC1_UNITS = 128\n",
    "config.CRITIC_FC2_UNITS = 64\n",
    "config.NOISE_THETA = 0.15\n",
    "config.NOISE_SIGMA = 0.2\n",
    "config.LR_ACTOR = 1e-4\n",
    "config.LR_CRITIC = 3e-4\n",
    "config.TAU = 1e-4\n",
    "\n",
    "#REPLAY BUFFER\n",
    "config.BUFFER_SIZE = int(1e6)\n",
    "config.BATCH_SIZE = 512\n",
    "config.GAMMA = 0.99\n",
    "\n",
    "config.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDDPG(ptan.agent.BaseAgent):\n",
    "\n",
    "    def __init__(self, net, device=\"cpu\", ou_enabled=True, ou_mu=0.0, ou_teta=0.15, ou_sigma=0.2, ou_epsilon=1.0):\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.ou_enabled = ou_enabled\n",
    "        self.ou_mu = ou_mu\n",
    "        self.ou_teta = ou_teta\n",
    "        self.ou_sigma = ou_sigma\n",
    "        self.ou_epsilon = ou_epsilon\n",
    "\n",
    "    def initial_state(self):\n",
    "        return None\n",
    "\n",
    "    def __call__(self, states, agent_states):\n",
    "        states_v = ptan.agent.float32_preprocessor(states).to(self.device)\n",
    "        mu_v = self.net(states_v)\n",
    "        actions = mu_v.data.cpu().numpy()\n",
    "\n",
    "        if self.ou_enabled and self.ou_epsilon > 0:\n",
    "            new_a_states = []\n",
    "            for a_state, action in zip(agent_states, actions):\n",
    "                if a_state is None:\n",
    "                    a_state = np.zeros(shape=action.shape, dtype=np.float32)\n",
    "                a_state += self.ou_teta * (self.ou_mu - a_state)\n",
    "                a_state += self.ou_sigma * np.random.normal(size=action.shape)\n",
    "\n",
    "                action += self.ou_epsilon * a_state\n",
    "                new_a_states.append(a_state)\n",
    "        else:\n",
    "            new_a_states = agent_states\n",
    "\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        return actions, new_a_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGActor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_units=400, fc2_units=300):\n",
    "        super(DDPGActor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm1d(state_size),\n",
    "            nn.Linear(state_size, fc1_units),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(fc1_units),\n",
    "            nn.Linear(fc1_units, fc2_units),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(fc2_units),\n",
    "            nn.Linear(fc2_units, action_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "class DDPGCritic(nn.Module):\n",
    "    def __init__(self,n_agents, state_size, action_size,fc1_units=400, fc2_units=300):\n",
    "        super(DDPGCritic, self).__init__()\n",
    "\n",
    "        self.obs_net = nn.Sequential(\n",
    "            nn.BatchNorm1d(n_agents*state_size),\n",
    "            nn.Linear(n_agents*state_size, fc1_units),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.out_net = nn.Sequential(\n",
    "            nn.BatchNorm1d(fc1_units + n_agents*action_size),\n",
    "            nn.Linear(fc1_units + n_agents*action_size, fc2_units),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(fc2_units),\n",
    "            nn.Linear(fc2_units, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        obs = self.obs_net(x)\n",
    "        return self.out_net(torch.cat([obs, a], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = DDPGActor(state_size,action_size,config.ACTOR_FC1_UNITS,config.ACTOR_FC2_UNITS)\n",
    "actor_target = ptan.agent.TargetNet(actor_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_net = DDPGCritic(n_agents, state_size,action_size,config.CRITIC_FC1_UNITS,config.CRITIC_FC2_UNITS)\n",
    "critic_target = ptan.agent.TargetNet(critic_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = optim.Adam(actor_net.parameters(),lr=config.LR_ACTOR)\n",
    "critic_optimizer = optim.Adam(critic_net.parameters(),lr=config.LR_CRITIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentDDPG(actor_net,device=config.device)\n",
    "test_agent = AgentDDPG(actor_net,device=config.device,ou_enabled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(config.BUFFER_SIZE,config.BATCH_SIZE,config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_replay_buffer(n_traj):\n",
    "    \n",
    "    agent_states = [None for _ in range(n_agents)]\n",
    "    \n",
    "    for _ in range(n_traj):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        for step in range(300):\n",
    "            states = env_info.vector_observations\n",
    "            actions,agent_states = agent(states,agent_states)\n",
    "            env_info= env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            dones = env_info.local_done\n",
    "            rewards = env_info.rewards            \n",
    "\n",
    "            replay_buffer.add(states,actions,rewards,next_states,dones)\n",
    "\n",
    "            if np.any(dones):                                \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay_buffer = ReplayBuffer(config.BUFFER_SIZE,5,config.device)\n",
    "#populate_replay_buffer(5)\n",
    "#states,actions,rewards,next_states,dones = replay_buffer.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 reward_100: 0.008900000136345626\n",
      "Epoch: 200 reward_100: 0.007800000142306089\n",
      "Epoch: 300 reward_100: 0.010900000166147948\n",
      "Epoch: 400 reward_100: 0.010000000149011612\n",
      "Epoch: 500 reward_100: 0.0020000000298023225\n",
      "Epoch: 594 reward: 0.00000000149011612\r"
     ]
    }
   ],
   "source": [
    "rewards_history = []\n",
    "stop_score = 0.5\n",
    "\n",
    "num_epochs = 50000\n",
    "max_steps = 300\n",
    "WAIT_EPOCHS = 300\n",
    "\n",
    "with ptan.common.utils.TBMeanTracker(writer, batch_size=100) as tb_tracker:\n",
    "    \n",
    "    agent_states = [None for _ in range(n_agents)]\n",
    "    \n",
    "    for epoch in range(1,num_epochs):\n",
    "\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        tot_reward = np.zeros(n_agents)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            states = env_info.vector_observations\n",
    "            actions,agent_states = agent(states,agent_states)\n",
    "            env_info= env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            dones = env_info.local_done\n",
    "            rewards = env_info.rewards\n",
    "            tot_reward += rewards\n",
    "\n",
    "            replay_buffer.add(states,actions,rewards,next_states,dones)\n",
    "\n",
    "            if np.any(dones):\n",
    "                max_reward = tot_reward.max()\n",
    "                tb_tracker.track(\"reward\", max_reward, epoch)               \n",
    "                rewards_history.append(max_reward)\n",
    "                \n",
    "                reward_100 = np.mean(rewards_history[-100:])\n",
    "                tb_tracker.track(\"reward_100\", reward_100, epoch)\n",
    "                \n",
    "                print(f'Epoch: {epoch} reward: {max_reward}',end='\\r')\n",
    "\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f'Epoch: {epoch} reward_100: {reward_100}')\n",
    "                \n",
    "                break\n",
    "\n",
    "            if len(replay_buffer) > config.BATCH_SIZE and epoch > WAIT_EPOCHS:\n",
    "            \n",
    "                states,actions,rewards,next_states,dones = replay_buffer.sample()\n",
    "                \n",
    "                #Actor View\n",
    "                s_v = states.view(-1,state_size)\n",
    "                a_v = actions.view(-1,action_size)\n",
    "                ns_v = next_states.view(-1,state_size)\n",
    "                r_v = rewards.view(-1,1)\n",
    "                done_v = dones.view(-1,1)\n",
    "                \n",
    "                #Critic View. \n",
    "                fs_v = states.view(-1,n_agents*state_size)                \n",
    "                fa_v = actions.view(-1,n_agents*action_size)                \n",
    "                fns_v = next_states.view(-1,n_agents*state_size)\n",
    "                fr_v = torch.from_numpy(rewards.detach().numpy().max(axis=1,keepdims=True))\n",
    "                fd_v = dones.any(axis = 1,keepdims=True)\n",
    "\n",
    "                #Calculate Q Targets and Loss\n",
    "                \n",
    "                fna_v = actor_target.model(ns_v).view(-1,n_agents*action_size)\n",
    "                q_next = critic_target.model(fns_v,fna_v)                \n",
    "                q_target = fr_v + config.GAMMA*q_next\n",
    "                \n",
    "                q_expected = critic_net(fs_v,fa_v*(1-fd_v))\n",
    "                critic_loss = F.mse_loss(q_expected, q_target)\n",
    "                \n",
    "                #Critic training step. clip gradients\n",
    "                critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(critic_net.parameters(), 1)\n",
    "                critic_optimizer.step()\n",
    "                \n",
    "                #Actor Training Step\n",
    "                actions_pred = actor_net(s_v).view(-1,n_agents*action_size)                \n",
    "                actor_loss = -critic_net(fs_v, actions_pred).mean()\n",
    "                \n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                actor_target.alpha_sync(alpha=1 - config.TAU)\n",
    "                critic_target.alpha_sync(alpha=1 - config.TAU)\n",
    "                \n",
    "                tb_tracker.track(\"loss_critic\", critic_loss, epoch)        \n",
    "                tb_tracker.track(\"loss_actor\", actor_loss, epoch)\n",
    "        \n",
    "       \n",
    "\n",
    "        if len(rewards_history) > 100 and reward_100 > stop_score:\n",
    "            print(f'Solved. Episode {epoch}, mean reward {reward_100}')\n",
    "            break\n",
    "\n",
    "\n",
    "    learning_state = {\n",
    "            'actor': actor_net.state_dict(),\n",
    "            'critic': critic_net.state_dict(),\n",
    "            'act_opt': actor_optimizer.state_dict(),\n",
    "            'crt_opt': critic_optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'rewards_history': rewards_history\n",
    "    }\n",
    "\n",
    "    torch.save(learning_state,'./partial_state.ckp')\n",
    "\n",
    "    writer.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_state = {\n",
    "        'actor': actor_net.state_dict(),\n",
    "        'critic': critic_net.state_dict(),\n",
    "        'act_opt': actor_optimizer.state_dict(),\n",
    "        'crt_opt': critic_optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'rewards_history': rewards_history\n",
    "}\n",
    "    \n",
    "torch.save(learning_state,'./partial_state.ckp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
